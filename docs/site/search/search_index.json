{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Stat \u00b6 Practical Statistics for Data Scientists Statistics 4e Statistical Models: Theory and Practice","title":"Home"},{"location":"PracticalStatisticsForDataScientists/","text":"Practical Statistics for Data Scientists \u00b6 ToC \u00b6 01. Exploratory Data Analysis [01.01. ][0101] 02. Data and Sampling Distributions \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"PracticalStatisticsForDataScientists/#toc","text":"01. Exploratory Data Analysis [01.01. ][0101] 02. Data and Sampling Distributions","title":"ToC"},{"location":"PracticalStatisticsForDataScientists/01/","text":"01. Exploratory Data Analysis \u00b6 ToC \u00b6 01.01. Elements of Structured Data 01.01.01. Further Reading 01.02. Rectangular Data 01.02.01. Data Frames and Indexed 01.02.02. Graph Data 01.02.03. Further Reading 01.03. Estimates of Location 01.03.01. Mean 01.03.02. Median and Robust Estimates 01.03.02.01. Outliers 01.03.03. Example: Location Estimates of Population and Murder Rates 01.03.04. Further Reading 01.04. Estimates of Variability 01.04.01. Standard Deviation and Related Estimates 01.04.02. Estimates Based on Percentiles 01.04.03. Example: Variability Estimated of State Population 01.05. Exploring the Data Distribution 01.05.01. Percentiles and Boxplots 01.05.02. Frequency Table and Histograms 01.05.03. Density Estimates 01.06. Exploring Binary and Categorical Data 01.07. Correlation 01.08. Exploring Two or More Variables 01.09. Conclusion 01.01. Elements of Structured Data \u00b6 numeric contiuous discrete categorical binary ordinal: ordered 01.01.01. Further Reading \u00b6 R Introduction / Basic data types SQL Data Types 01.02. Rectangular Data \u00b6 01.02.01. Data Frames and Indexed \u00b6 01.02.02. Graph Data \u00b6 01.02.03. Further Reading \u00b6 01.03. Estimates of Location \u00b6 01.03.01. Mean \u00b6 \\text{Mean} = \\bar{x} = \\frac{\\sum_i^N{x_i}}{N} trimmed mean p smallest and largest val omitted \\text{Trimmed Mean} = \\bar{x} = \\frac{\\sum_{i=p+1}^{N-p}{x_{(i)}}}{N-2p} weighted mean multiplying data val x_i by weight w_i & dividing sum of weights \\text{Weighted Mean} = \\bar{x}_w = \\frac{\\sum_{i=1}^N{w_i x_i}}{\\sum_i^N{w_i}} 01.03.02. Median and Robust Estimates \u00b6 weighted median 01.03.02.01. Outliers \u00b6 median: robust estimate of location since NOT influenced by outliers 01.03.03. Example: Location Estimates of Population and Murder Rates \u00b6 state <- read.csv(file = 'data/state.csv') mean(state[[\"Population\"]]) mean(state$Population) mean(state$Population, trim = .1) median(state$Population) weighted.mean(state$Murder.Rate, w = state$Population) library('matrixStats') weightedMedian(state$Murder.Rate, w = state$Population) 01.03.04. Further Reading \u00b6 01.04. Estimates of Variability \u00b6 variability , aka, dispersion Deviatons (errors, residuals) Variance (mean-squared-error) Standard Deviation (l2-norm, Euclidean norm) Mean Absolute Deviation (l1-norm, Manhattan norm) Median Absolute Deviation from the Median Range Order Statistics (ranks) Percentile Interquantile Range (IQR) 01.04.01. Standard Deviation and Related Estimates \u00b6 deviations \\text{Mean Absolution Deviation} = \\frac{\\sum_{i=1}^N{|x_i - \\bar{x}|}}{N} sample mean \\bar{x} variance , standard deviation \\begin{align*} \\text{Variance} &= s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{N-1} \\\\ \\text{Standard Deviation} &= s = \\sqrt{\\text{Variance}} \\end{align*} variance, sd, mean absolute deviation NOT robust robust estimate of variablity is median absolute deviation from the median (MAD) \\text{Median Absolution Deviation} = \\text{Median}(|x_1 - m|, |x_2 - m|, \\cdots, |x_N - m|) median m 01.04.02. Estimates Based on Percentiles \u00b6 order statistics range : diff between max & min percentile interquantile range : diff between 25th percentile & 75th percentile 01.04.03. Example: Variability Estimated of State Population \u00b6 sd(state$Population) IQR(state$Population) mad(state$Population) 01.04.04. Further Reading \u00b6 01.05. Exploring the Data Distribution \u00b6 moments 3rd moment skewness , 4th moment kurtosis 01.05.01. Percentiles and Boxplots \u00b6 quantiles (25th, 50th, 75th percentiles) deciles (10th, 20th, ..., 90th percentiles) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) boxplot(state$Population/1000000, ylab='Population (millions)') 01.05.02. Frequency Table and Histograms \u00b6 breaks <- seq(from=min(state$Population), to=max(state$Population), length=11) pop_freq <- cut(state$Population, breaks=breaks, right=TRUE, include.lowest=TRUE) table(pop_freq) hist(state$Population, breaks=breaks, main='Population', xlab='Population (millions)') 01.05.03. Density Estimates \u00b6 hist(state$Murder.Rate, freq=FALSE, main='Murder Rate', xlab='Murder Rate') lines(density(state$Murder.Rate), lwd=3, col='blue') 01.06. Exploring Binary and Categorical Data \u00b6 Mode : most commonly occurring category / val Expected Value Bar Charts Pie Charts \u00b6 img{width: 51%; float: right;}","title":"01. Exploratory Data Analysis"},{"location":"PracticalStatisticsForDataScientists/01/#toc","text":"01.01. Elements of Structured Data 01.01.01. Further Reading 01.02. Rectangular Data 01.02.01. Data Frames and Indexed 01.02.02. Graph Data 01.02.03. Further Reading 01.03. Estimates of Location 01.03.01. Mean 01.03.02. Median and Robust Estimates 01.03.02.01. Outliers 01.03.03. Example: Location Estimates of Population and Murder Rates 01.03.04. Further Reading 01.04. Estimates of Variability 01.04.01. Standard Deviation and Related Estimates 01.04.02. Estimates Based on Percentiles 01.04.03. Example: Variability Estimated of State Population 01.05. Exploring the Data Distribution 01.05.01. Percentiles and Boxplots 01.05.02. Frequency Table and Histograms 01.05.03. Density Estimates 01.06. Exploring Binary and Categorical Data 01.07. Correlation 01.08. Exploring Two or More Variables 01.09. Conclusion","title":"ToC"},{"location":"PracticalStatisticsForDataScientists/01/#0101_elements_of_structured_data","text":"numeric contiuous discrete categorical binary ordinal: ordered","title":"01.01. Elements of Structured Data"},{"location":"PracticalStatisticsForDataScientists/01/#010101_further_reading","text":"R Introduction / Basic data types SQL Data Types","title":"01.01.01. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0102_rectangular_data","text":"","title":"01.02. Rectangular Data"},{"location":"PracticalStatisticsForDataScientists/01/#010201_data_frames_and_indexed","text":"","title":"01.02.01. Data Frames and Indexed"},{"location":"PracticalStatisticsForDataScientists/01/#010202_graph_data","text":"","title":"01.02.02. Graph Data"},{"location":"PracticalStatisticsForDataScientists/01/#010203_further_reading","text":"","title":"01.02.03. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0103_estimates_of_location","text":"","title":"01.03. Estimates of Location"},{"location":"PracticalStatisticsForDataScientists/01/#010301_mean","text":"\\text{Mean} = \\bar{x} = \\frac{\\sum_i^N{x_i}}{N} trimmed mean p smallest and largest val omitted \\text{Trimmed Mean} = \\bar{x} = \\frac{\\sum_{i=p+1}^{N-p}{x_{(i)}}}{N-2p} weighted mean multiplying data val x_i by weight w_i & dividing sum of weights \\text{Weighted Mean} = \\bar{x}_w = \\frac{\\sum_{i=1}^N{w_i x_i}}{\\sum_i^N{w_i}}","title":"01.03.01. Mean"},{"location":"PracticalStatisticsForDataScientists/01/#010302_median_and_robust_estimates","text":"weighted median","title":"01.03.02. Median and Robust Estimates"},{"location":"PracticalStatisticsForDataScientists/01/#01030201_outliers","text":"median: robust estimate of location since NOT influenced by outliers","title":"01.03.02.01. Outliers"},{"location":"PracticalStatisticsForDataScientists/01/#010303_example_location_estimates_of_population_and_murder_rates","text":"state <- read.csv(file = 'data/state.csv') mean(state[[\"Population\"]]) mean(state$Population) mean(state$Population, trim = .1) median(state$Population) weighted.mean(state$Murder.Rate, w = state$Population) library('matrixStats') weightedMedian(state$Murder.Rate, w = state$Population)","title":"01.03.03. Example: Location Estimates of Population and Murder Rates"},{"location":"PracticalStatisticsForDataScientists/01/#010304_further_reading","text":"","title":"01.03.04. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0104_estimates_of_variability","text":"variability , aka, dispersion Deviatons (errors, residuals) Variance (mean-squared-error) Standard Deviation (l2-norm, Euclidean norm) Mean Absolute Deviation (l1-norm, Manhattan norm) Median Absolute Deviation from the Median Range Order Statistics (ranks) Percentile Interquantile Range (IQR)","title":"01.04. Estimates of Variability"},{"location":"PracticalStatisticsForDataScientists/01/#010401_standard_deviation_and_related_estimates","text":"deviations \\text{Mean Absolution Deviation} = \\frac{\\sum_{i=1}^N{|x_i - \\bar{x}|}}{N} sample mean \\bar{x} variance , standard deviation \\begin{align*} \\text{Variance} &= s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{N-1} \\\\ \\text{Standard Deviation} &= s = \\sqrt{\\text{Variance}} \\end{align*} variance, sd, mean absolute deviation NOT robust robust estimate of variablity is median absolute deviation from the median (MAD) \\text{Median Absolution Deviation} = \\text{Median}(|x_1 - m|, |x_2 - m|, \\cdots, |x_N - m|) median m","title":"01.04.01. Standard Deviation and Related Estimates"},{"location":"PracticalStatisticsForDataScientists/01/#010402_estimates_based_on_percentiles","text":"order statistics range : diff between max & min percentile interquantile range : diff between 25th percentile & 75th percentile","title":"01.04.02. Estimates Based on Percentiles"},{"location":"PracticalStatisticsForDataScientists/01/#010403_example_variability_estimated_of_state_population","text":"sd(state$Population) IQR(state$Population) mad(state$Population)","title":"01.04.03. Example: Variability Estimated of State Population"},{"location":"PracticalStatisticsForDataScientists/01/#010404_further_reading","text":"","title":"01.04.04. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0105_exploring_the_data_distribution","text":"moments 3rd moment skewness , 4th moment kurtosis","title":"01.05. Exploring the Data Distribution"},{"location":"PracticalStatisticsForDataScientists/01/#010501_percentiles_and_boxplots","text":"quantiles (25th, 50th, 75th percentiles) deciles (10th, 20th, ..., 90th percentiles) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) boxplot(state$Population/1000000, ylab='Population (millions)')","title":"01.05.01. Percentiles and Boxplots"},{"location":"PracticalStatisticsForDataScientists/01/#010502_frequency_table_and_histograms","text":"breaks <- seq(from=min(state$Population), to=max(state$Population), length=11) pop_freq <- cut(state$Population, breaks=breaks, right=TRUE, include.lowest=TRUE) table(pop_freq) hist(state$Population, breaks=breaks, main='Population', xlab='Population (millions)')","title":"01.05.02. Frequency Table and Histograms"},{"location":"PracticalStatisticsForDataScientists/01/#010503_density_estimates","text":"hist(state$Murder.Rate, freq=FALSE, main='Murder Rate', xlab='Murder Rate') lines(density(state$Murder.Rate), lwd=3, col='blue')","title":"01.05.03. Density Estimates"},{"location":"PracticalStatisticsForDataScientists/01/#0106_exploring_binary_and_categorical_data","text":"Mode : most commonly occurring category / val Expected Value Bar Charts Pie Charts","title":"01.06. Exploring Binary and Categorical Data"},{"location":"StatisticalModelsTheoryAndPractice/","text":"Statistical Models: Theory and Practice \u00b6 ToC \u00b6 01. Observational Studies and Experiments 02. The Regression Line 03. Matrix Algebra 04. Multiple Regression 05. Multiple Regreesion: Special Topics 06. Path Models 07. Maximum Likelihood 08. The Bootstrap 09. Simultaneous Equations 10. Issues in Statistical Modeling \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"StatisticalModelsTheoryAndPractice/#toc","text":"01. Observational Studies and Experiments 02. The Regression Line 03. Matrix Algebra 04. Multiple Regression 05. Multiple Regreesion: Special Topics 06. Path Models 07. Maximum Likelihood 08. The Bootstrap 09. Simultaneous Equations 10. Issues in Statistical Modeling","title":"ToC"},{"location":"StatisticalModelsTheoryAndPractice/01/","text":"01. Observational Studies and Experiments \u00b6 ToC \u00b6 01.01. Introduction 01.02. The HIP trial 01.03. Snow on cholera 01.04. Yule on the causes of poverty 01.01. Introduction \u00b6 regressin models used for to summarize data to predict future to predict the secults of interventions (caisal inferece) the key problem: confounding handled by subdividing the population ( stratification aka cross-tabulation ) modeling \"control\" control: subject NOT get treatment controlled experiment: study where the investigators decide who will be in the treatment group association as we control for more var, study groups get smaller, more room for chance effects problem w/ cross-tabulation \u21d2 use stat model 01.02. The HIP trial \u00b6 Health Insurance Plan intention-to-treat analysis 01.03. Snow on cholera \u00b6 natural experiment : observational study as if randomized by nature John Snow Broad street pump in Soho 01.04. Yule on the causes of poverty \u00b6 regression technique Legendre (1805), Gauss (1809) Yule (1899) \\Delta \\text{Paup} = a + b \\times \\Delta\\text{Out} + c \\times \\Delta\\text{Old} + d \\times \\Delta\\text{Pop} + \\text{error} \\tag{1} \\Delta percentaage change over time \\text{Paup} percentage of paupers \\text{Out} out-relief ration \\frac{N}{D} N = \\text{number on welfare outside the poor-house} D = \\text{number inside} \\text{Old} percentage of the population aged over 65 \\text{Pop} population \\sum{( \\Delta\\text{Paup} - a - b \\times \\Delta\\text{Out} - c \\times \\Delta\\text{Old} - d \\times \\Delta\\text{Pop} )^2} \\Delta \\text{Paup} = 13.19 + 0.755 \\Delta\\text{Out} - 0.022 \\Delta\\text{Old} - 0.322 \\Delta\\text{Pop} + \\text{error} \\tag{2} \\Delta \\text{Paup} = 1.36 + 0.324 \\Delta\\text{Out} + 1.37 \\Delta\\text{Old} - 0.369 \\Delta\\text{Pop} + \\text{error} \\tag{3} \\Delta\\text{Out} relatively large and pos \\begin{align*} \\Delta\\text{Out} &= 5 - 100 = -95 \\\\ \\Delta\\text{Old} &= 104 - 100 = 4 \\\\ \\Delta\\text{Pop} &= 136 - 100 = 36 \\end{align*} \\Delta\\text{Paup} 13.19 + 0.755 \\times (-95) - 0.022 \\times 4 - 0.322 \\times 36 = -70 actual val for \\Delta\\text{Paup} -73 , so \\text{error} -3 Quantitative inference : \\Delta\\text{Out} +1\\text{percent age pts} \u21d2 \\Delta\\text{Paup} +0.755\\text{pecentage pts} Qualitative inference : Out-relief causes an increase in pauperism \u00b6 img{width: 51%; float: right;}","title":"01. Observational Studies and Experiments"},{"location":"StatisticalModelsTheoryAndPractice/01/#toc","text":"01.01. Introduction 01.02. The HIP trial 01.03. Snow on cholera 01.04. Yule on the causes of poverty","title":"ToC"},{"location":"StatisticalModelsTheoryAndPractice/01/#0101_introduction","text":"regressin models used for to summarize data to predict future to predict the secults of interventions (caisal inferece) the key problem: confounding handled by subdividing the population ( stratification aka cross-tabulation ) modeling \"control\" control: subject NOT get treatment controlled experiment: study where the investigators decide who will be in the treatment group association as we control for more var, study groups get smaller, more room for chance effects problem w/ cross-tabulation \u21d2 use stat model","title":"01.01. Introduction"},{"location":"StatisticalModelsTheoryAndPractice/01/#0102_the_hip_trial","text":"Health Insurance Plan intention-to-treat analysis","title":"01.02. The HIP trial"},{"location":"StatisticalModelsTheoryAndPractice/01/#0103_snow_on_cholera","text":"natural experiment : observational study as if randomized by nature John Snow Broad street pump in Soho","title":"01.03. Snow on cholera"},{"location":"StatisticalModelsTheoryAndPractice/01/#0104_yule_on_the_causes_of_poverty","text":"regression technique Legendre (1805), Gauss (1809) Yule (1899) \\Delta \\text{Paup} = a + b \\times \\Delta\\text{Out} + c \\times \\Delta\\text{Old} + d \\times \\Delta\\text{Pop} + \\text{error} \\tag{1} \\Delta percentaage change over time \\text{Paup} percentage of paupers \\text{Out} out-relief ration \\frac{N}{D} N = \\text{number on welfare outside the poor-house} D = \\text{number inside} \\text{Old} percentage of the population aged over 65 \\text{Pop} population \\sum{( \\Delta\\text{Paup} - a - b \\times \\Delta\\text{Out} - c \\times \\Delta\\text{Old} - d \\times \\Delta\\text{Pop} )^2} \\Delta \\text{Paup} = 13.19 + 0.755 \\Delta\\text{Out} - 0.022 \\Delta\\text{Old} - 0.322 \\Delta\\text{Pop} + \\text{error} \\tag{2} \\Delta \\text{Paup} = 1.36 + 0.324 \\Delta\\text{Out} + 1.37 \\Delta\\text{Old} - 0.369 \\Delta\\text{Pop} + \\text{error} \\tag{3} \\Delta\\text{Out} relatively large and pos \\begin{align*} \\Delta\\text{Out} &= 5 - 100 = -95 \\\\ \\Delta\\text{Old} &= 104 - 100 = 4 \\\\ \\Delta\\text{Pop} &= 136 - 100 = 36 \\end{align*} \\Delta\\text{Paup} 13.19 + 0.755 \\times (-95) - 0.022 \\times 4 - 0.322 \\times 36 = -70 actual val for \\Delta\\text{Paup} -73 , so \\text{error} -3 Quantitative inference : \\Delta\\text{Out} +1\\text{percent age pts} \u21d2 \\Delta\\text{Paup} +0.755\\text{pecentage pts} Qualitative inference : Out-relief causes an increase in pauperism","title":"01.04. Yule on the causes of poverty"},{"location":"Statistics4e/","text":"Statistics 4e \u00b6 ToC \u00b6 Part I. Design of Experiments 01. Controlled Experiments 02. Observational Studies Part II. Descriptive Statistics 03. The Histogram 04. The Average and the Standard Deviation 05. The Normal Approximation for Data 06. Mesurement Error 07. Plotting Points and Lines Part III. Correlation and Regression 08. Correlation 09. More about Correlation 10. Regression 11. The R.M.S. Error for Regression 12. The Regression Line Part IV. Probability 13. What Are the Chances 14. More about Chance 15. The Binominal Formula Part V. Chance Variability 16. The Law of Averages 17. The Expected Value and Statndard Error 18. The Normal Approximation for Probability Histrogram Part VI. Sampling 19. Sample Sureys 20. Chance Error in Sampling 21. The Accuracy of Percentages 22. Measuring Employment and Unemployment 23. The Accuracy of Averages Part VII. Chance Models 24. A Model for Measurement Error 25. Chance Models in Genetics Part VIII. Tests of Significance 26. Tests of Significance 27. More Tests for Averages 28. The Chi-Square Test 29. A Closer Look at Tests of Significance \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"Statistics4e/#toc","text":"Part I. Design of Experiments 01. Controlled Experiments 02. Observational Studies Part II. Descriptive Statistics 03. The Histogram 04. The Average and the Standard Deviation 05. The Normal Approximation for Data 06. Mesurement Error 07. Plotting Points and Lines Part III. Correlation and Regression 08. Correlation 09. More about Correlation 10. Regression 11. The R.M.S. Error for Regression 12. The Regression Line Part IV. Probability 13. What Are the Chances 14. More about Chance 15. The Binominal Formula Part V. Chance Variability 16. The Law of Averages 17. The Expected Value and Statndard Error 18. The Normal Approximation for Probability Histrogram Part VI. Sampling 19. Sample Sureys 20. Chance Error in Sampling 21. The Accuracy of Percentages 22. Measuring Employment and Unemployment 23. The Accuracy of Averages Part VII. Chance Models 24. A Model for Measurement Error 25. Chance Models in Genetics Part VIII. Tests of Significance 26. Tests of Significance 27. More Tests for Averages 28. The Chi-Square Test 29. A Closer Look at Tests of Significance","title":"ToC"}]}