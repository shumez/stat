{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Stat \u00b6 Introductory Statistics Practical Statistics for Data Scientists Statistics 4e Statistical Models: Theory and Practice","title":"Home"},{"location":"CollaborativeStatistics/","text":"Collaborative Statistics \u00b6 ToC \u00b6 [05. Sampling and Data][05] [05.01. Sampling and Data][0501] [05.02. Statistics][0502] [05.03. Probability][0503] [05.04. Key Terms][0504] [05.05. Data][0505] [05.06. Sampling][0506] [05.07. Variation][0507] [05.08. Aswers and Roudning Off][0508] [05.09. Frequency][0509] [05.10. Summary][0510] [05.11. Practice: Sampling and Data][0511] [05.12. Homework][0512] [05.13. Lab 1: Data Collection][0513] [05.14. Lab 2: Sampling Experiment][0514] [06. Descriptive Statistics][06] [06.01. ][0601] [07. Probability Topics][07] [08. Discrete Random Variables][08] [09. Continuous Random Variables][09] \u00b6 \u00b6","title":"Intro"},{"location":"CollaborativeStatistics/#toc","text":"[05. Sampling and Data][05] [05.01. Sampling and Data][0501] [05.02. Statistics][0502] [05.03. Probability][0503] [05.04. Key Terms][0504] [05.05. Data][0505] [05.06. Sampling][0506] [05.07. Variation][0507] [05.08. Aswers and Roudning Off][0508] [05.09. Frequency][0509] [05.10. Summary][0510] [05.11. Practice: Sampling and Data][0511] [05.12. Homework][0512] [05.13. Lab 1: Data Collection][0513] [05.14. Lab 2: Sampling Experiment][0514] [06. Descriptive Statistics][06] [06.01. ][0601] [07. Probability Topics][07] [08. Discrete Random Variables][08] [09. Continuous Random Variables][09]","title":"ToC"},{"location":"IntroductoryStatistics/","text":"Introductory Statistics \u00b6 ToC \u00b6 01. Sampling and Data 01.01. Definitions of Statistics, Probability, and Key Terms [02. Descriptive Statistics][02] [03. Probability Topics][03] [04. Discrete Random Variables][04] [05. Continuous Random Variables][05] [06. The Normal Distribution][06] [07. The Central Limit Theorem][07] [08. Confidence Intervals][08] [09. Hypothesis Testing with One Sample][09] [10. Hypothesis Testing with Two Samples][10] [11. The Chi-Square Distribution][11] [12. Linear Regression and Correlation][12] [13. F Distribution and One-Way ANOVA][13] \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"IntroductoryStatistics/#toc","text":"01. Sampling and Data 01.01. Definitions of Statistics, Probability, and Key Terms [02. Descriptive Statistics][02] [03. Probability Topics][03] [04. Discrete Random Variables][04] [05. Continuous Random Variables][05] [06. The Normal Distribution][06] [07. The Central Limit Theorem][07] [08. Confidence Intervals][08] [09. Hypothesis Testing with One Sample][09] [10. Hypothesis Testing with Two Samples][10] [11. The Chi-Square Distribution][11] [12. Linear Regression and Correlation][12] [13. F Distribution and One-Way ANOVA][13]","title":"ToC"},{"location":"IntroductoryStatistics/01/","text":"01. Sampling and Data \u00b6 ToC \u00b6 01.01. Definitions of Statistics, Probability, and Key Terms Collaborative Exercise 01.01.01. Probability 01.01.02. Key Terms Example 1.1 Example 1.2 01.02. Data, Sampling, and Variation in Data and Sampling [Example 1.5][example15] 01.02.01. Qualitative Data Discussion 01.01. Definitions of Statistics, Probability, and Key Terms \u00b6 descriptive statistics inferential statistics Collaborative Exercise \u00b6 01.01.01. Probability \u00b6 Probability [Karl Pearson] 01.01.02. Key Terms \u00b6 Population Sample Sampling Mean Proportion Example 1.1 \u00b6 Example 1.2 \u00b6 01.02. Data, Sampling, and Variation in Data and Sampling \u00b6 qualitative categorical quantitative discrte continuous Example 1.5 \u00b6 Example 1.6 \u00b6 Example 1.7 \u00b6 Example 1.8 \u00b6 Example 1.9 \u00b6 Example 1.10 \u00b6 01.02.01. Qualitative Data Discussion \u00b6 \u00b6 img{width: 51%; float: right;}","title":"01. Sampling and Data"},{"location":"IntroductoryStatistics/01/#toc","text":"01.01. Definitions of Statistics, Probability, and Key Terms Collaborative Exercise 01.01.01. Probability 01.01.02. Key Terms Example 1.1 Example 1.2 01.02. Data, Sampling, and Variation in Data and Sampling [Example 1.5][example15] 01.02.01. Qualitative Data Discussion","title":"ToC"},{"location":"IntroductoryStatistics/01/#0101_definitions_of_statistics_probability_and_key_terms","text":"descriptive statistics inferential statistics","title":"01.01. Definitions of Statistics, Probability, and Key Terms"},{"location":"IntroductoryStatistics/01/#collaborative_exercise","text":"","title":"Collaborative Exercise"},{"location":"IntroductoryStatistics/01/#010101_probability","text":"Probability [Karl Pearson]","title":"01.01.01. Probability"},{"location":"IntroductoryStatistics/01/#010102_key_terms","text":"Population Sample Sampling Mean Proportion","title":"01.01.02. Key Terms"},{"location":"IntroductoryStatistics/01/#example_11","text":"","title":"Example 1.1"},{"location":"IntroductoryStatistics/01/#example_12","text":"","title":"Example 1.2"},{"location":"IntroductoryStatistics/01/#0102_data_sampling_and_variation_in_data_and_sampling","text":"qualitative categorical quantitative discrte continuous","title":"01.02. Data, Sampling, and Variation in Data and Sampling"},{"location":"IntroductoryStatistics/01/#example_15","text":"","title":"Example 1.5"},{"location":"IntroductoryStatistics/01/#example_16","text":"","title":"Example 1.6"},{"location":"IntroductoryStatistics/01/#example_17","text":"","title":"Example 1.7"},{"location":"IntroductoryStatistics/01/#example_18","text":"","title":"Example 1.8"},{"location":"IntroductoryStatistics/01/#example_19","text":"","title":"Example 1.9"},{"location":"IntroductoryStatistics/01/#example_110","text":"","title":"Example 1.10"},{"location":"IntroductoryStatistics/01/#010201_qualitative_data_discussion","text":"","title":"01.02.01. Qualitative Data Discussion"},{"location":"PracticalStatisticsForDataScientists/","text":"Practical Statistics for Data Scientists \u00b6 ToC \u00b6 01. Exploratory Data Analysis [01.01. ][0101] 02. Data and Sampling Distributions \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"PracticalStatisticsForDataScientists/#toc","text":"01. Exploratory Data Analysis [01.01. ][0101] 02. Data and Sampling Distributions","title":"ToC"},{"location":"PracticalStatisticsForDataScientists/01/","text":"01. Exploratory Data Analysis \u00b6 ToC \u00b6 01.01. Elements of Structured Data 01.01.01. Further Reading 01.02. Rectangular Data 01.02.01. Data Frames and Indexed 01.02.02. Graph Data 01.02.03. Further Reading 01.03. Estimates of Location 01.03.01. Mean 01.03.02. Median and Robust Estimates 01.03.02.01. Outliers 01.03.03. Example: Location Estimates of Population and Murder Rates 01.03.04. Further Reading 01.04. Estimates of Variability 01.04.01. Standard Deviation and Related Estimates 01.04.02. Estimates Based on Percentiles 01.04.03. Example: Variability Estimated of State Population 01.05. Exploring the Data Distribution 01.05.01. Percentiles and Boxplots 01.05.02. Frequency Table and Histograms 01.05.03. Density Estimates 01.06. Exploring Binary and Categorical Data 01.06.01. Mode 01.06.02. Expected Value 01.06.03 Further Reading 01.07. Correlation 01.07.01. Scatterplots 01.08. Exploring Two or More Variables 01.08.01. Hexagonal Binning and Contours (plotting numeric vs. numeric) 01.08.02. Two Categorical Variables 01.08.03. Categorical and Numerical Data 01.08.04. Visualizing Multiple Variables 01.08.05. Further Reading 01.09. Conclusion 01.09. Conclusion 01.01. Elements of Structured Data \u00b6 numeric contiuous discrete categorical binary ordinal: ordered 01.01.01. Further Reading \u00b6 R Introduction / Basic data types SQL Data Types 01.02. Rectangular Data \u00b6 01.02.01. Data Frames and Indexed \u00b6 01.02.02. Graph Data \u00b6 01.02.03. Further Reading \u00b6 01.03. Estimates of Location \u00b6 01.03.01. Mean \u00b6 \\text{Mean} = \\bar{x} = \\frac{\\sum_i^N{x_i}}{N} trimmed mean p smallest and largest val omitted \\text{Trimmed Mean} = \\bar{x} = \\frac{\\sum_{i=p+1}^{N-p}{x_{(i)}}}{N-2p} weighted mean multiplying data val x_i by weight w_i & dividing sum of weights \\text{Weighted Mean} = \\bar{x}_w = \\frac{\\sum_{i=1}^N{w_i x_i}}{\\sum_i^N{w_i}} 01.03.02. Median and Robust Estimates \u00b6 weighted median 01.03.02.01. Outliers \u00b6 median: robust estimate of location since NOT influenced by outliers 01.03.03. Example: Location Estimates of Population and Murder Rates \u00b6 state <- read.csv(file = 'data/state.csv') mean(state[[\"Population\"]]) mean(state$Population) mean(state$Population, trim = .1) median(state$Population) weighted.mean(state$Murder.Rate, w = state$Population) library('matrixStats') weightedMedian(state$Murder.Rate, w = state$Population) 01.03.04. Further Reading \u00b6 01.04. Estimates of Variability \u00b6 variability , aka, dispersion Deviatons (errors, residuals) Variance (mean-squared-error) Standard Deviation (l2-norm, Euclidean norm) Mean Absolute Deviation (l1-norm, Manhattan norm) Median Absolute Deviation from the Median Range Order Statistics (ranks) Percentile Interquantile Range (IQR) 01.04.01. Standard Deviation and Related Estimates \u00b6 deviations \\text{Mean Absolution Deviation} = \\frac{\\sum_{i=1}^N{|x_i - \\bar{x}|}}{N} sample mean \\bar{x} variance , standard deviation \\begin{align*} \\text{Variance} &= s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{N-1} \\\\ \\text{Standard Deviation} &= s = \\sqrt{\\text{Variance}} \\end{align*} variance, sd, mean absolute deviation NOT robust robust estimate of variablity is median absolute deviation from the median (MAD) \\text{Median Absolution Deviation} = \\text{Median}(|x_1 - m|, |x_2 - m|, \\cdots, |x_N - m|) median m 01.04.02. Estimates Based on Percentiles \u00b6 order statistics range : diff between max & min percentile interquantile range : diff between 25th percentile & 75th percentile 01.04.03. Example: Variability Estimated of State Population \u00b6 sd(state$Population) IQR(state$Population) mad(state$Population) 01.04.04. Further Reading \u00b6 01.05. Exploring the Data Distribution \u00b6 moments 3rd moment skewness , 4th moment kurtosis 01.05.01. Percentiles and Boxplots \u00b6 quantiles (25th, 50th, 75th percentiles) deciles (10th, 20th, ..., 90th percentiles) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) boxplot(state$Population/1000000, ylab='Population (millions)') 01.05.02. Frequency Table and Histograms \u00b6 breaks <- seq(from=min(state$Population), to=max(state$Population), length=11) pop_freq <- cut(state$Population, breaks=breaks, right=TRUE, include.lowest=TRUE) table(pop_freq) hist(state$Population, breaks=breaks, main='Population', xlab='Population (millions)') 01.05.03. Density Estimates \u00b6 hist(state$Murder.Rate, freq=FALSE, main='Murder Rate', xlab='Murder Rate') lines(density(state$Murder.Rate), lwd=3, col='blue') 01.06. Exploring Binary and Categorical Data \u00b6 Mode : most commonly occurring category / val Expected Value Bar Charts Pie Charts 01.06.01. Mode \u00b6 mode : simple summary for categorical data 01.06.02. Expected Value \u00b6 \\text{EV} = 0.05 \\times 300 + 0.15 \\times 50 + 0.80 \\times 0 = 22.5 01.06.03 Further Reading \u00b6 misleading graphs 01.07. Correlation \u00b6 Correlation coefficient : Correlation mat Sctter plot Alchemy r = \\frac{\\sum_{i=1}^N{(x_i - \\bar{x}) (y_i - \\bar{y})}}{(N-1) s_x s_y} 01.07.01. Scatterplots \u00b6 01.08. Exploring Two or More Variables \u00b6 Contingency Tables Hexagonal Binning Contour Plots Violin Plots 01.08.01. Hexagonal Binning and Contours (plotting numeric vs. numeric) \u00b6 01.08.02. Two Categorical Variables \u00b6 01.08.03. Categorical and Numerical Data \u00b6 01.08.04. Visualizing Multiple Variables \u00b6 01.08.05. Further Reading \u00b6 01.09. Conclusion \u00b6 \u00b6 img{width: 51%; float: right;}","title":"01. Exploratory Data Analysis"},{"location":"PracticalStatisticsForDataScientists/01/#toc","text":"01.01. Elements of Structured Data 01.01.01. Further Reading 01.02. Rectangular Data 01.02.01. Data Frames and Indexed 01.02.02. Graph Data 01.02.03. Further Reading 01.03. Estimates of Location 01.03.01. Mean 01.03.02. Median and Robust Estimates 01.03.02.01. Outliers 01.03.03. Example: Location Estimates of Population and Murder Rates 01.03.04. Further Reading 01.04. Estimates of Variability 01.04.01. Standard Deviation and Related Estimates 01.04.02. Estimates Based on Percentiles 01.04.03. Example: Variability Estimated of State Population 01.05. Exploring the Data Distribution 01.05.01. Percentiles and Boxplots 01.05.02. Frequency Table and Histograms 01.05.03. Density Estimates 01.06. Exploring Binary and Categorical Data 01.06.01. Mode 01.06.02. Expected Value 01.06.03 Further Reading 01.07. Correlation 01.07.01. Scatterplots 01.08. Exploring Two or More Variables 01.08.01. Hexagonal Binning and Contours (plotting numeric vs. numeric) 01.08.02. Two Categorical Variables 01.08.03. Categorical and Numerical Data 01.08.04. Visualizing Multiple Variables 01.08.05. Further Reading 01.09. Conclusion 01.09. Conclusion","title":"ToC"},{"location":"PracticalStatisticsForDataScientists/01/#0101_elements_of_structured_data","text":"numeric contiuous discrete categorical binary ordinal: ordered","title":"01.01. Elements of Structured Data"},{"location":"PracticalStatisticsForDataScientists/01/#010101_further_reading","text":"R Introduction / Basic data types SQL Data Types","title":"01.01.01. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0102_rectangular_data","text":"","title":"01.02. Rectangular Data"},{"location":"PracticalStatisticsForDataScientists/01/#010201_data_frames_and_indexed","text":"","title":"01.02.01. Data Frames and Indexed"},{"location":"PracticalStatisticsForDataScientists/01/#010202_graph_data","text":"","title":"01.02.02. Graph Data"},{"location":"PracticalStatisticsForDataScientists/01/#010203_further_reading","text":"","title":"01.02.03. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0103_estimates_of_location","text":"","title":"01.03. Estimates of Location"},{"location":"PracticalStatisticsForDataScientists/01/#010301_mean","text":"\\text{Mean} = \\bar{x} = \\frac{\\sum_i^N{x_i}}{N} trimmed mean p smallest and largest val omitted \\text{Trimmed Mean} = \\bar{x} = \\frac{\\sum_{i=p+1}^{N-p}{x_{(i)}}}{N-2p} weighted mean multiplying data val x_i by weight w_i & dividing sum of weights \\text{Weighted Mean} = \\bar{x}_w = \\frac{\\sum_{i=1}^N{w_i x_i}}{\\sum_i^N{w_i}}","title":"01.03.01. Mean"},{"location":"PracticalStatisticsForDataScientists/01/#010302_median_and_robust_estimates","text":"weighted median","title":"01.03.02. Median and Robust Estimates"},{"location":"PracticalStatisticsForDataScientists/01/#01030201_outliers","text":"median: robust estimate of location since NOT influenced by outliers","title":"01.03.02.01. Outliers"},{"location":"PracticalStatisticsForDataScientists/01/#010303_example_location_estimates_of_population_and_murder_rates","text":"state <- read.csv(file = 'data/state.csv') mean(state[[\"Population\"]]) mean(state$Population) mean(state$Population, trim = .1) median(state$Population) weighted.mean(state$Murder.Rate, w = state$Population) library('matrixStats') weightedMedian(state$Murder.Rate, w = state$Population)","title":"01.03.03. Example: Location Estimates of Population and Murder Rates"},{"location":"PracticalStatisticsForDataScientists/01/#010304_further_reading","text":"","title":"01.03.04. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0104_estimates_of_variability","text":"variability , aka, dispersion Deviatons (errors, residuals) Variance (mean-squared-error) Standard Deviation (l2-norm, Euclidean norm) Mean Absolute Deviation (l1-norm, Manhattan norm) Median Absolute Deviation from the Median Range Order Statistics (ranks) Percentile Interquantile Range (IQR)","title":"01.04. Estimates of Variability"},{"location":"PracticalStatisticsForDataScientists/01/#010401_standard_deviation_and_related_estimates","text":"deviations \\text{Mean Absolution Deviation} = \\frac{\\sum_{i=1}^N{|x_i - \\bar{x}|}}{N} sample mean \\bar{x} variance , standard deviation \\begin{align*} \\text{Variance} &= s^2 = \\frac{\\sum{(x - \\bar{x})^2}}{N-1} \\\\ \\text{Standard Deviation} &= s = \\sqrt{\\text{Variance}} \\end{align*} variance, sd, mean absolute deviation NOT robust robust estimate of variablity is median absolute deviation from the median (MAD) \\text{Median Absolution Deviation} = \\text{Median}(|x_1 - m|, |x_2 - m|, \\cdots, |x_N - m|) median m","title":"01.04.01. Standard Deviation and Related Estimates"},{"location":"PracticalStatisticsForDataScientists/01/#010402_estimates_based_on_percentiles","text":"order statistics range : diff between max & min percentile interquantile range : diff between 25th percentile & 75th percentile","title":"01.04.02. Estimates Based on Percentiles"},{"location":"PracticalStatisticsForDataScientists/01/#010403_example_variability_estimated_of_state_population","text":"sd(state$Population) IQR(state$Population) mad(state$Population)","title":"01.04.03. Example: Variability Estimated of State Population"},{"location":"PracticalStatisticsForDataScientists/01/#010404_further_reading","text":"","title":"01.04.04. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0105_exploring_the_data_distribution","text":"moments 3rd moment skewness , 4th moment kurtosis","title":"01.05. Exploring the Data Distribution"},{"location":"PracticalStatisticsForDataScientists/01/#010501_percentiles_and_boxplots","text":"quantiles (25th, 50th, 75th percentiles) deciles (10th, 20th, ..., 90th percentiles) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) boxplot(state$Population/1000000, ylab='Population (millions)')","title":"01.05.01. Percentiles and Boxplots"},{"location":"PracticalStatisticsForDataScientists/01/#010502_frequency_table_and_histograms","text":"breaks <- seq(from=min(state$Population), to=max(state$Population), length=11) pop_freq <- cut(state$Population, breaks=breaks, right=TRUE, include.lowest=TRUE) table(pop_freq) hist(state$Population, breaks=breaks, main='Population', xlab='Population (millions)')","title":"01.05.02. Frequency Table and Histograms"},{"location":"PracticalStatisticsForDataScientists/01/#010503_density_estimates","text":"hist(state$Murder.Rate, freq=FALSE, main='Murder Rate', xlab='Murder Rate') lines(density(state$Murder.Rate), lwd=3, col='blue')","title":"01.05.03. Density Estimates"},{"location":"PracticalStatisticsForDataScientists/01/#0106_exploring_binary_and_categorical_data","text":"Mode : most commonly occurring category / val Expected Value Bar Charts Pie Charts","title":"01.06. Exploring Binary and Categorical Data"},{"location":"PracticalStatisticsForDataScientists/01/#010601_mode","text":"mode : simple summary for categorical data","title":"01.06.01. Mode"},{"location":"PracticalStatisticsForDataScientists/01/#010602_expected_value","text":"\\text{EV} = 0.05 \\times 300 + 0.15 \\times 50 + 0.80 \\times 0 = 22.5","title":"01.06.02. Expected Value"},{"location":"PracticalStatisticsForDataScientists/01/#010603_further_reading","text":"misleading graphs","title":"01.06.03 Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0107_correlation","text":"Correlation coefficient : Correlation mat Sctter plot Alchemy r = \\frac{\\sum_{i=1}^N{(x_i - \\bar{x}) (y_i - \\bar{y})}}{(N-1) s_x s_y}","title":"01.07. Correlation"},{"location":"PracticalStatisticsForDataScientists/01/#010701_scatterplots","text":"","title":"01.07.01. Scatterplots"},{"location":"PracticalStatisticsForDataScientists/01/#0108_exploring_two_or_more_variables","text":"Contingency Tables Hexagonal Binning Contour Plots Violin Plots","title":"01.08. Exploring Two or More Variables"},{"location":"PracticalStatisticsForDataScientists/01/#010801_hexagonal_binning_and_contours_plotting_numeric_vs_numeric","text":"","title":"01.08.01. Hexagonal Binning and Contours (plotting numeric vs. numeric)"},{"location":"PracticalStatisticsForDataScientists/01/#010802_two_categorical_variables","text":"","title":"01.08.02. Two Categorical Variables"},{"location":"PracticalStatisticsForDataScientists/01/#010803_categorical_and_numerical_data","text":"","title":"01.08.03. Categorical and Numerical Data"},{"location":"PracticalStatisticsForDataScientists/01/#010804_visualizing_multiple_variables","text":"","title":"01.08.04. Visualizing Multiple Variables"},{"location":"PracticalStatisticsForDataScientists/01/#010805_further_reading","text":"","title":"01.08.05. Further Reading"},{"location":"PracticalStatisticsForDataScientists/01/#0109_conclusion","text":"","title":"01.09. Conclusion"},{"location":"StatisticalModelsTheoryAndPractice/","text":"Statistical Models: Theory and Practice \u00b6 ToC \u00b6 01. Observational Studies and Experiments 02. The Regression Line 03. Matrix Algebra 04. Multiple Regression 05. Multiple Regreesion: Special Topics 06. Path Models 07. Maximum Likelihood 08. The Bootstrap 09. Simultaneous Equations 10. Issues in Statistical Modeling \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"StatisticalModelsTheoryAndPractice/#toc","text":"01. Observational Studies and Experiments 02. The Regression Line 03. Matrix Algebra 04. Multiple Regression 05. Multiple Regreesion: Special Topics 06. Path Models 07. Maximum Likelihood 08. The Bootstrap 09. Simultaneous Equations 10. Issues in Statistical Modeling","title":"ToC"},{"location":"StatisticalModelsTheoryAndPractice/01/","text":"01. Observational Studies and Experiments \u00b6 ToC \u00b6 01.01. Introduction 01.02. The HIP trial 01.03. Snow on cholera 01.04. Yule on the causes of poverty 01.01. Introduction \u00b6 regressin models used for to summarize data to predict future to predict the secults of interventions (caisal inferece) the key problem: confounding handled by subdividing the population ( stratification aka cross-tabulation ) modeling \"control\" control: subject NOT get treatment controlled experiment: study where the investigators decide who will be in the treatment group association as we control for more var, study groups get smaller, more room for chance effects problem w/ cross-tabulation \u21d2 use stat model 01.02. The HIP trial \u00b6 Health Insurance Plan intention-to-treat analysis 01.03. Snow on cholera \u00b6 natural experiment : observational study as if randomized by nature John Snow Broad street pump in Soho 01.04. Yule on the causes of poverty \u00b6 regression technique Legendre (1805), Gauss (1809) Yule (1899) \\Delta \\text{Paup} = a + b \\times \\Delta\\text{Out} + c \\times \\Delta\\text{Old} + d \\times \\Delta\\text{Pop} + \\text{error} \\tag{1} \\Delta percentaage change over time \\text{Paup} percentage of paupers \\text{Out} out-relief ration \\frac{N}{D} N = \\text{number on welfare outside the poor-house} D = \\text{number inside} \\text{Old} percentage of the population aged over 65 \\text{Pop} population \\sum{( \\Delta\\text{Paup} - a - b \\times \\Delta\\text{Out} - c \\times \\Delta\\text{Old} - d \\times \\Delta\\text{Pop} )^2} \\Delta \\text{Paup} = 13.19 + 0.755 \\Delta\\text{Out} - 0.022 \\Delta\\text{Old} - 0.322 \\Delta\\text{Pop} + \\text{error} \\tag{2} \\Delta \\text{Paup} = 1.36 + 0.324 \\Delta\\text{Out} + 1.37 \\Delta\\text{Old} - 0.369 \\Delta\\text{Pop} + \\text{error} \\tag{3} \\Delta\\text{Out} relatively large and pos \\begin{align*} \\Delta\\text{Out} &= 5 - 100 = -95 \\\\ \\Delta\\text{Old} &= 104 - 100 = 4 \\\\ \\Delta\\text{Pop} &= 136 - 100 = 36 \\end{align*} \\Delta\\text{Paup} 13.19 + 0.755 \\times (-95) - 0.022 \\times 4 - 0.322 \\times 36 = -70 actual val for \\Delta\\text{Paup} -73 , so \\text{error} -3 Quantitative inference : \\Delta\\text{Out} +1\\text{percent age pts} \u21d2 \\Delta\\text{Paup} +0.755\\text{pecentage pts} Qualitative inference : Out-relief causes an increase in pauperism \u00b6 img{width: 51%; float: right;}","title":"01. Observational Studies and Experiments"},{"location":"StatisticalModelsTheoryAndPractice/01/#toc","text":"01.01. Introduction 01.02. The HIP trial 01.03. Snow on cholera 01.04. Yule on the causes of poverty","title":"ToC"},{"location":"StatisticalModelsTheoryAndPractice/01/#0101_introduction","text":"regressin models used for to summarize data to predict future to predict the secults of interventions (caisal inferece) the key problem: confounding handled by subdividing the population ( stratification aka cross-tabulation ) modeling \"control\" control: subject NOT get treatment controlled experiment: study where the investigators decide who will be in the treatment group association as we control for more var, study groups get smaller, more room for chance effects problem w/ cross-tabulation \u21d2 use stat model","title":"01.01. Introduction"},{"location":"StatisticalModelsTheoryAndPractice/01/#0102_the_hip_trial","text":"Health Insurance Plan intention-to-treat analysis","title":"01.02. The HIP trial"},{"location":"StatisticalModelsTheoryAndPractice/01/#0103_snow_on_cholera","text":"natural experiment : observational study as if randomized by nature John Snow Broad street pump in Soho","title":"01.03. Snow on cholera"},{"location":"StatisticalModelsTheoryAndPractice/01/#0104_yule_on_the_causes_of_poverty","text":"regression technique Legendre (1805), Gauss (1809) Yule (1899) \\Delta \\text{Paup} = a + b \\times \\Delta\\text{Out} + c \\times \\Delta\\text{Old} + d \\times \\Delta\\text{Pop} + \\text{error} \\tag{1} \\Delta percentaage change over time \\text{Paup} percentage of paupers \\text{Out} out-relief ration \\frac{N}{D} N = \\text{number on welfare outside the poor-house} D = \\text{number inside} \\text{Old} percentage of the population aged over 65 \\text{Pop} population \\sum{( \\Delta\\text{Paup} - a - b \\times \\Delta\\text{Out} - c \\times \\Delta\\text{Old} - d \\times \\Delta\\text{Pop} )^2} \\Delta \\text{Paup} = 13.19 + 0.755 \\Delta\\text{Out} - 0.022 \\Delta\\text{Old} - 0.322 \\Delta\\text{Pop} + \\text{error} \\tag{2} \\Delta \\text{Paup} = 1.36 + 0.324 \\Delta\\text{Out} + 1.37 \\Delta\\text{Old} - 0.369 \\Delta\\text{Pop} + \\text{error} \\tag{3} \\Delta\\text{Out} relatively large and pos \\begin{align*} \\Delta\\text{Out} &= 5 - 100 = -95 \\\\ \\Delta\\text{Old} &= 104 - 100 = 4 \\\\ \\Delta\\text{Pop} &= 136 - 100 = 36 \\end{align*} \\Delta\\text{Paup} 13.19 + 0.755 \\times (-95) - 0.022 \\times 4 - 0.322 \\times 36 = -70 actual val for \\Delta\\text{Paup} -73 , so \\text{error} -3 Quantitative inference : \\Delta\\text{Out} +1\\text{percent age pts} \u21d2 \\Delta\\text{Paup} +0.755\\text{pecentage pts} Qualitative inference : Out-relief causes an increase in pauperism","title":"01.04. Yule on the causes of poverty"},{"location":"Statistics4e/","text":"Statistics 4e \u00b6 ToC \u00b6 Part I. Design of Experiments 01. Controlled Experiments 02. Observational Studies Part II. Descriptive Statistics 03. The Histogram 04. The Average and the Standard Deviation 05. The Normal Approximation for Data 06. Mesurement Error 07. Plotting Points and Lines Part III. Correlation and Regression 08. Correlation 09. More about Correlation 10. Regression 11. The R.M.S. Error for Regression 12. The Regression Line Part IV. Probability 13. What Are the Chances 14. More about Chance 15. The Binominal Formula Part V. Chance Variability 16. The Law of Averages 17. The Expected Value and Statndard Error 18. The Normal Approximation for Probability Histrogram Part VI. Sampling 19. Sample Sureys 20. Chance Error in Sampling 21. The Accuracy of Percentages 22. Measuring Employment and Unemployment 23. The Accuracy of Averages Part VII. Chance Models 24. A Model for Measurement Error 25. Chance Models in Genetics Part VIII. Tests of Significance 26. Tests of Significance 27. More Tests for Averages 28. The Chi-Square Test 29. A Closer Look at Tests of Significance \u00b6 \u00b6 img{width: 51%; float: right;}","title":"Intro"},{"location":"Statistics4e/#toc","text":"Part I. Design of Experiments 01. Controlled Experiments 02. Observational Studies Part II. Descriptive Statistics 03. The Histogram 04. The Average and the Standard Deviation 05. The Normal Approximation for Data 06. Mesurement Error 07. Plotting Points and Lines Part III. Correlation and Regression 08. Correlation 09. More about Correlation 10. Regression 11. The R.M.S. Error for Regression 12. The Regression Line Part IV. Probability 13. What Are the Chances 14. More about Chance 15. The Binominal Formula Part V. Chance Variability 16. The Law of Averages 17. The Expected Value and Statndard Error 18. The Normal Approximation for Probability Histrogram Part VI. Sampling 19. Sample Sureys 20. Chance Error in Sampling 21. The Accuracy of Percentages 22. Measuring Employment and Unemployment 23. The Accuracy of Averages Part VII. Chance Models 24. A Model for Measurement Error 25. Chance Models in Genetics Part VIII. Tests of Significance 26. Tests of Significance 27. More Tests for Averages 28. The Chi-Square Test 29. A Closer Look at Tests of Significance","title":"ToC"}]}